{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will clean text after reading it in\n",
    "def clean(line, punctuation, stemmer):\n",
    "    #remove punctuation\n",
    "    line = str(line)\n",
    "    line2 = line.translate(str.maketrans('','', punctuation))\n",
    "\n",
    "    #stem words\n",
    "    line3 = [stemmer.stem(word) for word in line2.split()]\n",
    "\n",
    "    return \" \".join(line3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = re.sub('[$%<=>]', \"\", string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#read strings from dataset file\n",
    "filepath = 'financial-news-dataset-master/dataset.txt'\n",
    "\n",
    "with open(filepath, 'r') as f:\n",
    "    corpus = [clean(line, punctuation, stemmer) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_stats = pd.read_excel('statistics_reddit_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Sent140data/train.csv',  encoding = \"ISO-8859-1\", header = None)\n",
    "test = pd.read_csv('Sent140data/test.csv',  encoding = \"ISO-8859-1\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(sent[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_reddit = []\n",
    "\n",
    "for post in reddit_stats['comment_body']:\n",
    "    clean_reddit.append([clean(post, punctuation, stemmer), 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    clean_tweets.append(clean(tweet, punctuation, stemmer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_class = []\n",
    "\n",
    "for tweet in clean_tweets:\n",
    "    tweets_class.append([tweet, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600498"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_class = []\n",
    "\n",
    "for doc in corpus:\n",
    "    corpus_class.append([doc, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= tweets_class + corpus_class + clean_reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a df with clean_tweets and reuters data\n",
    "DF = pd.DataFrame(df, columns = ['text','class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = DF['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features = 250,\n",
    "                             stop_words = 'english')\n",
    "X = vectorizer.fit_transform(DF['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for feature extraction\n",
    "sorted_vocab = sorted(vectorizer.vocabulary_.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 250,\n",
    "                             max_features= 100, \n",
    "                             max_depth=25, n_jobs=-1, random_state=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=25, max_features=100, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=-1,\n",
       "            oob_score=False, random_state=420, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9926130411261825"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model oos prediction\n",
    "accuracy_score(y_test, y_predict)\n",
    "precision_score(y_test, y_predict, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.367, ('report', 185)),\n",
      " (0.2395, ('said', 195)),\n",
      " (0.1457, ('edit', 63)),\n",
      " (0.0671, ('percent', 166)),\n",
      " (0.033, ('market', 142)),\n",
      " (0.0308, ('data', 46)),\n",
      " (0.0165, ('compani', 34)),\n",
      " (0.0158, ('year', 247)),\n",
      " (0.0124, ('billion', 19)),\n",
      " (0.0077, ('use', 233)),\n",
      " (0.0076, ('thi', 218)),\n",
      " (0.0052, ('ha', 98)),\n",
      " (0.0038, ('valu', 234)),\n",
      " (0.0035, ('financi', 81)),\n",
      " (0.0029, ('estim', 65)),\n",
      " (0.002, ('gener', 90)),\n",
      " (0.0019, ('bank', 13)),\n",
      " (0.0018, ('point', 169)),\n",
      " (0.0014, ('share', 204)),\n",
      " (0.0014, ('result', 186)),\n",
      " (0.0013, ('peopl', 165)),\n",
      " (0.0012, ('becaus', 14)),\n",
      " (0.0011, ('reuter', 187)),\n",
      " (0.0009, ('problem', 175)),\n",
      " (0.0008, ('wa', 236)),\n",
      " (0.0008, ('level', 127)),\n",
      " (0.0008, ('industri', 117)),\n",
      " (0.0007, ('think', 220)),\n",
      " (0.0007, ('say', 197)),\n",
      " (0.0007, ('new', 152)),\n",
      " (0.0007, ('million', 145)),\n",
      " (0.0007, ('mean', 143)),\n",
      " (0.0007, ('expect', 71)),\n",
      " (0.0007, ('ani', 8)),\n",
      " (0.0007, ('addit', 3)),\n",
      " (0.0006, ('make', 139)),\n",
      " (0.0006, ('like', 128)),\n",
      " (0.0006, ('case', 25)),\n",
      " (0.0005, ('lot', 135)),\n",
      " (0.0005, ('govern', 94)),\n",
      " (0.0005, ('analyst', 7)),\n",
      " (0.0004, ('work', 245)),\n",
      " (0.0004, ('veri', 235)),\n",
      " (0.0004, ('unit', 232)),\n",
      " (0.0004, ('sourc', 209)),\n",
      " (0.0004, ('econom', 61)),\n",
      " (0.0003, ('time', 222)),\n",
      " (0.0003, ('post', 172)),\n",
      " (0.0003, ('number', 155)),\n",
      " (0.0003, ('mani', 141)),\n",
      " (0.0003, ('just', 124)),\n",
      " (0.0003, ('job', 123)),\n",
      " (0.0003, ('comment', 33)),\n",
      " (0.0003, ('chief', 29)),\n",
      " (0.0003, ('amp', 6)),\n",
      " (0.0002, ('york', 248)),\n",
      " (0.0002, ('way', 240)),\n",
      " (0.0002, ('want', 238)),\n",
      " (0.0002, ('twitter', 231)),\n",
      " (0.0002, ('thing', 219)),\n",
      " (0.0002, ('thank', 217)),\n",
      " (0.0002, ('state', 212)),\n",
      " (0.0002, ('realli', 181)),\n",
      " (0.0002, ('rate', 180)),\n",
      " (0.0002, ('price', 174)),\n",
      " (0.0002, ('plan', 167)),\n",
      " (0.0002, ('onli', 161)),\n",
      " (0.0002, ('look', 133)),\n",
      " (0.0002, ('lol', 131)),\n",
      " (0.0002, ('know', 125)),\n",
      " (0.0002, ('includ', 114)),\n",
      " (0.0002, ('im', 113)),\n",
      " (0.0002, ('good', 92)),\n",
      " (0.0002, ('economi', 62)),\n",
      " (0.0002, ('dont', 56)),\n",
      " (0.0002, ('better', 17)),\n",
      " (0.0002, ('agre', 4)),\n",
      " (0.0001, ('world', 246)),\n",
      " (0.0001, ('whi', 243)),\n",
      " (0.0001, ('week', 242)),\n",
      " (0.0001, ('wednesday', 241)),\n",
      " (0.0001, ('wait', 237)),\n",
      " (0.0001, ('tuesday', 230)),\n",
      " (0.0001, ('tri', 229)),\n",
      " (0.0001, ('told', 224)),\n",
      " (0.0001, ('today', 223)),\n",
      " (0.0001, ('thursday', 221)),\n",
      " (0.0001, ('talk', 215)),\n",
      " (0.0001, ('start', 211)),\n",
      " (0.0001, ('sorri', 208)),\n",
      " (0.0001, ('sinc', 205)),\n",
      " (0.0001, ('set', 203)),\n",
      " (0.0001, ('sector', 199)),\n",
      " (0.0001, ('sad', 194)),\n",
      " (0.0001, ('right', 189)),\n",
      " (0.0001, ('presid', 173)),\n",
      " (0.0001, ('posit', 171)),\n",
      " (0.0001, ('oh', 159)),\n",
      " (0.0001, ('need', 151)),\n",
      " (0.0001, ('morn', 150)),\n",
      " (0.0001, ('miss', 146)),\n",
      " (0.0001, ('major', 138)),\n",
      " (0.0001, ('love', 136)),\n",
      " (0.0001, ('ive', 122)),\n",
      " (0.0001, ('issu', 121)),\n",
      " (0.0001, ('increas', 115)),\n",
      " (0.0001, ('ill', 112)),\n",
      " (0.0001, ('hour', 110)),\n",
      " (0.0001, ('hope', 109)),\n",
      " (0.0001, ('higher', 105)),\n",
      " (0.0001, ('hi', 103)),\n",
      " (0.0001, ('help', 102)),\n",
      " (0.0001, ('haha', 99)),\n",
      " (0.0001, ('group', 96)),\n",
      " (0.0001, ('great', 95)),\n",
      " (0.0001, ('got', 93)),\n",
      " (0.0001, ('fund', 88)),\n",
      " (0.0001, ('fun', 87)),\n",
      " (0.0001, ('friday', 85)),\n",
      " (0.0001, ('follow', 83)),\n",
      " (0.0001, ('financ', 80)),\n",
      " (0.0001, ('feel', 77)),\n",
      " (0.0001, ('feder', 76)),\n",
      " (0.0001, ('execut', 70)),\n",
      " (0.0001, ('euro', 66)),\n",
      " (0.0001, ('end', 64)),\n",
      " (0.0001, ('doe', 54)),\n",
      " (0.0001, ('didnt', 53)),\n",
      " (0.0001, ('did', 52)),\n",
      " (0.0001, ('debt', 49)),\n",
      " (0.0001, ('deal', 48)),\n",
      " (0.0001, ('day', 47)),\n",
      " (0.0001, ('current', 44)),\n",
      " (0.0001, ('credit', 42)),\n",
      " (0.0001, ('corp', 38)),\n",
      " (0.0001, ('come', 32)),\n",
      " (0.0001, ('chang', 28)),\n",
      " (0.0001, ('central', 27)),\n",
      " (0.0001, ('busi', 21)),\n",
      " (0.0001, ('befor', 15)),\n",
      " (0.0001, ('bad', 12)),\n",
      " (0.0001, ('averag', 11)),\n",
      " (0.0001, ('anoth', 9)),\n",
      " (0.0, ('zone', 249)),\n",
      " (0.0, ('wish', 244)),\n",
      " (0.0, ('watch', 239)),\n",
      " (0.0, ('trade', 228)),\n",
      " (0.0, ('total', 227)),\n",
      " (0.0, ('tonight', 226)),\n",
      " (0.0, ('tomorrow', 225)),\n",
      " (0.0, ('tax', 216)),\n",
      " (0.0, ('support', 214)),\n",
      " (0.0, ('stock', 213)),\n",
      " (0.0, ('spend', 210)),\n",
      " (0.0, ('soon', 207)),\n",
      " (0.0, ('sleep', 206)),\n",
      " (0.0, ('servic', 202)),\n",
      " (0.0, ('sell', 201)),\n",
      " (0.0, ('secur', 200)),\n",
      " (0.0, ('second', 198)),\n",
      " (0.0, ('sale', 196)),\n",
      " (0.0, ('run', 193)),\n",
      " (0.0, ('rose', 192)),\n",
      " (0.0, ('risk', 191)),\n",
      " (0.0, ('rise', 190)),\n",
      " (0.0, ('revenu', 188)),\n",
      " (0.0, ('remain', 184)),\n",
      " (0.0, ('regul', 183)),\n",
      " (0.0, ('recent', 182)),\n",
      " (0.0, ('rais', 179)),\n",
      " (0.0, ('quarter', 178)),\n",
      " (0.0, ('profit', 177)),\n",
      " (0.0, ('product', 176)),\n",
      " (0.0, ('polici', 170)),\n",
      " (0.0, ('play', 168)),\n",
      " (0.0, ('pay', 164)),\n",
      " (0.0, ('order', 163)),\n",
      " (0.0, ('oper', 162)),\n",
      " (0.0, ('oil', 160)),\n",
      " (0.0, ('offici', 158)),\n",
      " (0.0, ('offic', 157)),\n",
      " (0.0, ('offer', 156)),\n",
      " (0.0, ('night', 154)),\n",
      " (0.0, ('nice', 153)),\n",
      " (0.0, ('month', 149)),\n",
      " (0.0, ('money', 148)),\n",
      " (0.0, ('monday', 147)),\n",
      " (0.0, ('meet', 144)),\n",
      " (0.0, ('manag', 140)),\n",
      " (0.0, ('low', 137)),\n",
      " (0.0, ('loss', 134)),\n",
      " (0.0, ('long', 132)),\n",
      " (0.0, ('loan', 130)),\n",
      " (0.0, ('littl', 129)),\n",
      " (0.0, ('late', 126)),\n",
      " (0.0, ('investor', 120)),\n",
      " (0.0, ('invest', 119)),\n",
      " (0.0, ('intern', 118)),\n",
      " (0.0, ('index', 116)),\n",
      " (0.0, ('hous', 111)),\n",
      " (0.0, ('home', 108)),\n",
      " (0.0, ('hold', 107)),\n",
      " (0.0, ('hit', 106)),\n",
      " (0.0, ('high', 104)),\n",
      " (0.0, ('head', 101)),\n",
      " (0.0, ('happi', 100)),\n",
      " (0.0, ('growth', 97)),\n",
      " (0.0, ('global', 91)),\n",
      " (0.0, ('gain', 89)),\n",
      " (0.0, ('friend', 86)),\n",
      " (0.0, ('forecast', 84)),\n",
      " (0.0, ('firm', 82)),\n",
      " (0.0, ('final', 79)),\n",
      " (0.0, ('fell', 78)),\n",
      " (0.0, ('fed', 75)),\n",
      " (0.0, ('far', 74)),\n",
      " (0.0, ('fall', 73)),\n",
      " (0.0, ('face', 72)),\n",
      " (0.0, ('exchang', 69)),\n",
      " (0.0, ('european', 68)),\n",
      " (0.0, ('europ', 67)),\n",
      " (0.0, ('earn', 60)),\n",
      " (0.0, ('earlier', 59)),\n",
      " (0.0, ('earli', 58)),\n",
      " (0.0, ('drop', 57)),\n",
      " (0.0, ('dollar', 55)),\n",
      " (0.0, ('demand', 51)),\n",
      " (0.0, ('declin', 50)),\n",
      " (0.0, ('cut', 45)),\n",
      " (0.0, ('crisi', 43)),\n",
      " (0.0, ('court', 41)),\n",
      " (0.0, ('countri', 40)),\n",
      " (0.0, ('cost', 39)),\n",
      " (0.0, ('continu', 37)),\n",
      " (0.0, ('consum', 36)),\n",
      " (0.0, ('concern', 35)),\n",
      " (0.0, ('close', 31)),\n",
      " (0.0, ('china', 30)),\n",
      " (0.0, ('cent', 26)),\n",
      " (0.0, ('car', 24)),\n",
      " (0.0, ('capit', 23)),\n",
      " (0.0, ('buy', 22)),\n",
      " (0.0, ('bond', 20)),\n",
      " (0.0, ('big', 18)),\n",
      " (0.0, ('best', 16)),\n",
      " (0.0, ('asset', 10)),\n",
      " (0.0, ('alreadi', 5)),\n",
      " (0.0, ('ad', 2)),\n",
      " (0.0, ('accord', 1)),\n",
      " (0.0, ('10', 0))]\n"
     ]
    }
   ],
   "source": [
    "#get feature importances, sorted\n",
    "pprint(sorted(zip(map(lambda x: round(x, 4), clf.feature_importances_), sorted_vocab), reverse=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
